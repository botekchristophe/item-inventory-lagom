play.crypto.secret = "items"
play.application.loader = ca.cbotek.item.impl.ItemApplicationLoader
cassandra-query-journal.eventual-consistency-delay = 0s

lagom.cluster.join-self=on

service {
  item-inventory-entity {
    name = "item-inventory"
  }
}

cassandra {
  keyspace = "items"
  authentication {
    username = ""
    password = ""
  }
  consistency = "QUORUM"
  replication {
    strategy = "SimpleStrategy"
    factor = 1
  }
}

cassandra-journal {
  keyspace = ${cassandra.keyspace}
  authentication = ${cassandra.authentication}
  replication-strategy = ${cassandra.replication.strategy}
  replication-factor = ${cassandra.replication.factor}
  write-consistency = ${cassandra.consistency}
  read-consistency = ${cassandra.consistency}
}

cassandra-snapshot-store {
  keyspace = ${cassandra.keyspace}
  authentication = ${cassandra.authentication}
  replication-strategy = ${cassandra.replication.strategy}
  replication-factor = ${cassandra.replication.factor}
  write-consistency = ${cassandra.consistency}
  read-consistency = ${cassandra.consistency}
}

lagom.persistence.read-side.cassandra {
  keyspace = ${cassandra.keyspace}
  authentication = ${cassandra.authentication}
  session-provider = com.lightbend.lagom.internal.persistence.cassandra.ServiceLocatorSessionProvider
}

akka.persistence.journal.plugin = "cassandra-journal"
akka.persistence.snapshot-store.plugin = "cassandra-snapshot-store"

lagom.persistence {

  # As a rule of thumb, the number of shards should be a factor ten greater
  # than the planned maximum number of cluster nodes. Less shards than number
  # of nodes will result in that some nodes will not host any shards. Too many
  # shards will result in less efficient management of the shards, e.g.
  # rebalancing overhead, and increased latency because the coordinator is
  # involved in the routing of the first message for each shard. The value
  # must be the same on all nodes in a running cluster. It can be changed
  # after stopping all nodes in the cluster.
  max-number-of-shards = 10

  # Persistent entities saves snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # It may be configured to "off" to disable snapshots.
  snapshot-after = 15

  # A persistent entity is passivated automatically if it does not receive
  # any messages during this timeout. Passivation is performed to reduce
  # memory consumption. Objects referenced by the entity can be garbage
  # collected after passivation. Next message will activate the entity
  # again, which will recover its state from persistent storage. Set to 0
  # to disable passivation - this should only be done when the number of
  # entities is bounded and their state, sharded across the cluster, will
  # fit in memory.
  passivate-after-idle-timeout = 120s

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  # The entities can still be accessed from other nodes.
  run-entities-on-role = ""

  # Default timeout for PersistentEntityRef.ask replies.
  ask-timeout = 10s

  dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 16
    }
    throughput = 1
  }
}